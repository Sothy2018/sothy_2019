scala> val model = RandomForest.trainClassifier(d_train, numClasses, categoricalFeaturesInfo,
     |   numTrees, featureSubsetStrategy, impurity, maxDepth, maxBins)
[Stage 33:>                                                                                                                             (0 + 32) / 41]15/04/18 02:59:20 ERROR Executor: Exception in task 6.0 in stage 33.0 (TID 1283)
java.lang.OutOfMemoryError: GC overhead limit exceeded
        at scala.collection.immutable.HashMap$HashMap1.get0(HashMap.scala:183)
        at scala.collection.immutable.HashMap$HashTrieMap.get0(HashMap.scala:314)
        at scala.collection.immutable.HashMap$HashTrieMap.get0(HashMap.scala:310)
        at scala.collection.immutable.HashMap.get(HashMap.scala:51)
        at scala.collection.MapLike$class.getOrElse(MapLike.scala:126)
        at scala.collection.AbstractMap.getOrElse(Map.scala:58)
        at org.apache.spark.mllib.tree.DecisionTree$$anonfun$org$apache$spark$mllib$tree$DecisionTree$$binSeqOp$1$1.apply(DecisionTree.scala:532)
        at org.apache.spark.mllib.tree.DecisionTree$$anonfun$org$apache$spark$mllib$tree$DecisionTree$$binSeqOp$1$1.apply(DecisionTree.scala:529)
        at scala.collection.immutable.HashMap$HashMap1.foreach(HashMap.scala:224)
        at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:403)
        at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:403)
        at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:403)
        at org.apache.spark.mllib.tree.DecisionTree$.org$apache$spark$mllib$tree$DecisionTree$$binSeqOp$1(DecisionTree.scala:529)
        at org.apache.spark.mllib.tree.DecisionTree$$anonfun$6$$anonfun$apply$8.apply(DecisionTree.scala:624)
        at org.apache.spark.mllib.tree.DecisionTree$$anonfun$6$$anonfun$apply$8.apply(DecisionTree.scala:624)
        at scala.collection.Iterator$class.foreach(Iterator.scala:727)
        at org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)
        at org.apache.spark.mllib.tree.DecisionTree$$anonfun$6.apply(DecisionTree.scala:624)
        at org.apache.spark.mllib.tree.DecisionTree$$anonfun$6.apply(DecisionTree.scala:613)
        at org.apache.spark.rdd.RDD$$anonfun$14.apply(RDD.scala:634)
        at org.apache.spark.rdd.RDD$$anonfun$14.apply(RDD.scala:634)
        at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
        at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:68)
        at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
        at org.apache.spark.scheduler.Task.run(Task.scala:64)
        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:203)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:745)
15/04/18 02:59:20 ERROR SparkUncaughtExceptionHandler: Uncaught exception in thread Thread[Executor task launch worker-18,5,main]
java.lang.OutOfMemoryError: GC overhead limit exceeded
        at scala.collection.immutable.HashMap$HashMap1.get0(HashMap.scala:183)
        at scala.collection.immutable.HashMap$HashTrieMap.get0(HashMap.scala:314)
        at scala.collection.immutable.HashMap$HashTrieMap.get0(HashMap.scala:310)
        at scala.collection.immutable.HashMap.get(HashMap.scala:51)
        at scala.collection.MapLike$class.getOrElse(MapLike.scala:126)
        at scala.collection.AbstractMap.getOrElse(Map.scala:58)
        at org.apache.spark.mllib.tree.DecisionTree$$anonfun$org$apache$spark$mllib$tree$DecisionTree$$binSeqOp$1$1.apply(DecisionTree.scala:532)
        at org.apache.spark.mllib.tree.DecisionTree$$anonfun$org$apache$spark$mllib$tree$DecisionTree$$binSeqOp$1$1.apply(DecisionTree.scala:529)
        at scala.collection.immutable.HashMap$HashMap1.foreach(HashMap.scala:224)
        at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:403)
        at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:403)
        at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:403)
        at org.apache.spark.mllib.tree.DecisionTree$.org$apache$spark$mllib$tree$DecisionTree$$binSeqOp$1(DecisionTree.scala:529)
        at org.apache.spark.mllib.tree.DecisionTree$$anonfun$6$$anonfun$apply$8.apply(DecisionTree.scala:624)
        at org.apache.spark.mllib.tree.DecisionTree$$anonfun$6$$anonfun$apply$8.apply(DecisionTree.scala:624)
        at scala.collection.Iterator$class.foreach(Iterator.scala:727)
        at org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)
        at org.apache.spark.mllib.tree.DecisionTree$$anonfun$6.apply(DecisionTree.scala:624)
        at org.apache.spark.mllib.tree.DecisionTree$$anonfun$6.apply(DecisionTree.scala:613)
        at org.apache.spark.rdd.RDD$$anonfun$14.apply(RDD.scala:634)
        at org.apache.spark.rdd.RDD$$anonfun$14.apply(RDD.scala:634)
        at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
        at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:68)
        at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
        at org.apache.spark.scheduler.Task.run(Task.scala:64)
        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:203)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:745)
15/04/18 02:59:20 WARN TaskSetManager: Lost task 6.0 in stage 33.0 (TID 1283, localhost): java.lang.OutOfMemoryError: GC overhead limit exceeded
        at scala.collection.immutable.HashMap$HashMap1.get0(HashMap.scala:183)
        at scala.collection.immutable.HashMap$HashTrieMap.get0(HashMap.scala:314)
        at scala.collection.immutable.HashMap$HashTrieMap.get0(HashMap.scala:310)
        at scala.collection.immutable.HashMap.get(HashMap.scala:51)
        at scala.collection.MapLike$class.getOrElse(MapLike.scala:126)
        at scala.collection.AbstractMap.getOrElse(Map.scala:58)
        at org.apache.spark.mllib.tree.DecisionTree$$anonfun$org$apache$spark$mllib$tree$DecisionTree$$binSeqOp$1$1.apply(DecisionTree.scala:532)
        at org.apache.spark.mllib.tree.DecisionTree$$anonfun$org$apache$spark$mllib$tree$DecisionTree$$binSeqOp$1$1.apply(DecisionTree.scala:529)
        at scala.collection.immutable.HashMap$HashMap1.foreach(HashMap.scala:224)
        at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:403)
        at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:403)
        at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:403)
        at org.apache.spark.mllib.tree.DecisionTree$.org$apache$spark$mllib$tree$DecisionTree$$binSeqOp$1(DecisionTree.scala:529)
        at org.apache.spark.mllib.tree.DecisionTree$$anonfun$6$$anonfun$apply$8.apply(DecisionTree.scala:624)
        at org.apache.spark.mllib.tree.DecisionTree$$anonfun$6$$anonfun$apply$8.apply(DecisionTree.scala:624)
        at scala.collection.Iterator$class.foreach(Iterator.scala:727)
        at org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)
        at org.apache.spark.mllib.tree.DecisionTree$$anonfun$6.apply(DecisionTree.scala:624)
        at org.apache.spark.mllib.tree.DecisionTree$$anonfun$6.apply(DecisionTree.scala:613)
        at org.apache.spark.rdd.RDD$$anonfun$14.apply(RDD.scala:634)
        at org.apache.spark.rdd.RDD$$anonfun$14.apply(RDD.scala:634)
        at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
        at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:68)
        at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
        at org.apache.spark.scheduler.Task.run(Task.scala:64)
        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:203)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:745)

15/04/18 02:59:20 ERROR TaskSetManager: Task 6 in stage 33.0 failed 1 times; aborting job
15/04/18 02:59:20 ERROR Executor: Exception in task 28.0 in stage 33.0 (TID 1305)
java.io.FileNotFoundException: /tmp/spark-b105bf22-9cb9-4278-9e36-ff0b5f51c3dd/blockmgr-8a816f41-302e-4ae9-8078-f1f53af01915/2b/shuffle_14_28_0.data (No such file or directory)
        at java.io.FileOutputStream.open(Native Method)
        at java.io.FileOutputStream.<init>(FileOutputStream.java:221)
        at org.apache.spark.storage.DiskBlockObjectWriter.open(BlockObjectWriter.scala:130)
        at org.apache.spark.storage.DiskBlockObjectWriter.write(BlockObjectWriter.scala:201)
        at org.apache.spark.util.collection.ExternalSorter$$anonfun$writePartitionedFile$5$$anonfun$apply$2.apply(ExternalSorter.scala:754)
        at org.apache.spark.util.collection.ExternalSorter$$anonfun$writePartitionedFile$5$$anonfun$apply$2.apply(ExternalSorter.scala:753)
        at scala.collection.Iterator$class.foreach(Iterator.scala:727)
        at org.apache.spark.util.collection.ExternalSorter$IteratorForPartition.foreach(ExternalSorter.scala:818)
        at org.apache.spark.util.collection.ExternalSorter$$anonfun$writePartitionedFile$5.apply(ExternalSorter.scala:753)
        at org.apache.spark.util.collection.ExternalSorter$$anonfun$writePartitionedFile$5.apply(ExternalSorter.scala:749)
        at scala.collection.Iterator$class.foreach(Iterator.scala:727)
        at scala.collection.AbstractIterator.foreach(Iterator.scala:1157)
        at org.apache.spark.util.collection.ExternalSorter.writePartitionedFile(ExternalSorter.scala:749)
        at org.apache.spark.shuffle.sort.SortShuffleWriter.write(SortShuffleWriter.scala:68)
        at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:68)
        at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
        at org.apache.spark.scheduler.Task.run(Task.scala:64)
        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:203)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:745)
15/04/18 02:59:20 ERROR Executor: Exception in task 5.0 in stage 33.0 (TID 1282)
java.io.FileNotFoundException: /tmp/spark-b105bf22-9cb9-4278-9e36-ff0b5f51c3dd/blockmgr-8a816f41-302e-4ae9-8078-f1f53af01915/38/shuffle_14_5_0.data (No such file or directory)
        at java.io.FileOutputStream.open(Native Method)
        at java.io.FileOutputStream.<init>(FileOutputStream.java:221)
        at org.apache.spark.storage.DiskBlockObjectWriter.open(BlockObjectWriter.scala:130)
        at org.apache.spark.storage.DiskBlockObjectWriter.write(BlockObjectWriter.scala:201)
        at org.apache.spark.util.collection.ExternalSorter$$anonfun$writePartitionedFile$5$$anonfun$apply$2.apply(ExternalSorter.scala:754)
        at org.apache.spark.util.collection.ExternalSorter$$anonfun$writePartitionedFile$5$$anonfun$apply$2.apply(ExternalSorter.scala:753)
        at scala.collection.Iterator$class.foreach(Iterator.scala:727)
        at org.apache.spark.util.collection.ExternalSorter$IteratorForPartition.foreach(ExternalSorter.scala:818)
        at org.apache.spark.util.collection.ExternalSorter$$anonfun$writePartitionedFile$5.apply(ExternalSorter.scala:753)
        at org.apache.spark.util.collection.ExternalSorter$$anonfun$writePartitionedFile$5.apply(ExternalSorter.scala:749)
        at scala.collection.Iterator$class.foreach(Iterator.scala:727)
        at scala.collection.AbstractIterator.foreach(Iterator.scala:1157)
        at org.apache.spark.util.collection.ExternalSorter.writePartitionedFile(ExternalSorter.scala:749)
        at org.apache.spark.shuffle.sort.SortShuffleWriter.write(SortShuffleWriter.scala:68)
        at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:68)
        at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
        at org.apache.spark.scheduler.Task.run(Task.scala:64)
        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:203)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:745)
15/04/18 02:59:20 WARN TaskSetManager: Lost task 5.0 in stage 33.0 (TID 1282, localhost): java.io.FileNotFoundException: /tmp/spark-b105bf22-9cb9-4278-9e36-ff0b5f51c3dd/blockmgr-8a816f41-302e-4ae9-8078-f1f53af01915/38/shuffle_14_5_0.data (No such file or directory)
        at java.io.FileOutputStream.open(Native Method)
        at java.io.FileOutputStream.<init>(FileOutputStream.java:221)
        at org.apache.spark.storage.DiskBlockObjectWriter.open(BlockObjectWriter.scala:130)
        at org.apache.spark.storage.DiskBlockObjectWriter.write(BlockObjectWriter.scala:201)
        at org.apache.spark.util.collection.ExternalSorter$$anonfun$writePartitionedFile$5$$anonfun$apply$2.apply(ExternalSorter.scala:754)
        at org.apache.spark.util.collection.ExternalSorter$$anonfun$writePartitionedFile$5$$anonfun$apply$2.apply(ExternalSorter.scala:753)
        at scala.collection.Iterator$class.foreach(Iterator.scala:727)
        at org.apache.spark.util.collection.ExternalSorter$IteratorForPartition.foreach(ExternalSorter.scala:818)
        at org.apache.spark.util.collection.ExternalSorter$$anonfun$writePartitionedFile$5.apply(ExternalSorter.scala:753)
        at org.apache.spark.util.collection.ExternalSorter$$anonfun$writePartitionedFile$5.apply(ExternalSorter.scala:749)
        at scala.collection.Iterator$class.foreach(Iterator.scala:727)
        at scala.collection.AbstractIterator.foreach(Iterator.scala:1157)
        at org.apache.spark.util.collection.ExternalSorter.writePartitionedFile(ExternalSorter.scala:749)
        at org.apache.spark.shuffle.sort.SortShuffleWriter.write(SortShuffleWriter.scala:68)
        at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:68)
        at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
        at org.apache.spark.scheduler.Task.run(Task.scala:64)
        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:203)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:745)

15/04/18 02:59:20 WARN TaskSetManager: Lost task 28.0 in stage 33.0 (TID 1305, localhost): java.io.FileNotFoundException: /tmp/spark-b105bf22-9cb9-4278-9e36-ff0b5f51c3dd/blockmgr-8a816f41-302e-4ae9-8078-f1f53af01915/2b/shuffle_14_28_0.data (No such file or directory)
        at java.io.FileOutputStream.open(Native Method)
        at java.io.FileOutputStream.<init>(FileOutputStream.java:221)
        at org.apache.spark.storage.DiskBlockObjectWriter.open(BlockObjectWriter.scala:130)
        at org.apache.spark.storage.DiskBlockObjectWriter.write(BlockObjectWriter.scala:201)
        at org.apache.spark.util.collection.ExternalSorter$$anonfun$writePartitionedFile$5$$anonfun$apply$2.apply(ExternalSorter.scala:754)
        at org.apache.spark.util.collection.ExternalSorter$$anonfun$writePartitionedFile$5$$anonfun$apply$2.apply(ExternalSorter.scala:753)
        at scala.collection.Iterator$class.foreach(Iterator.scala:727)
        at org.apache.spark.util.collection.ExternalSorter$IteratorForPartition.foreach(ExternalSorter.scala:818)
        at org.apache.spark.util.collection.ExternalSorter$$anonfun$writePartitionedFile$5.apply(ExternalSorter.scala:753)
        at org.apache.spark.util.collection.ExternalSorter$$anonfun$writePartitionedFile$5.apply(ExternalSorter.scala:749)
        at scala.collection.Iterator$class.foreach(Iterator.scala:727)
        at scala.collection.AbstractIterator.foreach(Iterator.scala:1157)
        at org.apache.spark.util.collection.ExternalSorter.writePartitionedFile(ExternalSorter.scala:749)
        at org.apache.spark.shuffle.sort.SortShuffleWriter.write(SortShuffleWriter.scala:68)
        at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:68)
        at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
        at org.apache.spark.scheduler.Task.run(Task.scala:64)
        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:203)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:745)

15/04/18 02:59:20 WARN TaskSetManager: Lost task 14.0 in stage 33.0 (TID 1291, localhost): TaskKilled (killed intentionally)
15/04/18 02:59:20 WARN TaskSetManager: Lost task 12.0 in stage 33.0 (TID 1289, localhost): TaskKilled (killed intentionally)
15/04/18 02:59:20 WARN TaskSetManager: Lost task 22.0 in stage 33.0 (TID 1299, localhost): TaskKilled (killed intentionally)
15/04/18 02:59:20 WARN TaskSetManager: Lost task 4.0 in stage 33.0 (TID 1281, localhost): TaskKilled (killed intentionally)
15/04/18 02:59:20 WARN TaskSetManager: Lost task 18.0 in stage 33.0 (TID 1295, localhost): TaskKilled (killed intentionally)
15/04/18 02:59:20 WARN TaskSetManager: Lost task 8.0 in stage 33.0 (TID 1285, localhost): TaskKilled (killed intentionally)
15/04/18 02:59:20 WARN TaskSetManager: Lost task 29.0 in stage 33.0 (TID 1306, localhost): TaskKilled (killed intentionally)
15/04/18 02:59:20 WARN TaskSetManager: Lost task 27.0 in stage 33.0 (TID 1304, localhost): TaskKilled (killed intentionally)
15/04/18 02:59:20 WARN TaskSetManager: Lost task 20.0 in stage 33.0 (TID 1297, localhost): TaskKilled (killed intentionally)
15/04/18 02:59:20 WARN TaskSetManager: Lost task 1.0 in stage 33.0 (TID 1278, localhost): TaskKilled (killed intentionally)
15/04/18 02:59:20 WARN TaskSetManager: Lost task 26.0 in stage 33.0 (TID 1303, localhost): TaskKilled (killed intentionally)
15/04/18 02:59:20 WARN TaskSetManager: Lost task 24.0 in stage 33.0 (TID 1301, localhost): TaskKilled (killed intentionally)
15/04/18 02:59:20 WARN TaskSetManager: Lost task 10.0 in stage 33.0 (TID 1287, localhost): TaskKilled (killed intentionally)
15/04/18 02:59:20 WARN TaskSetManager: Lost task 11.0 in stage 33.0 (TID 1288, localhost): TaskKilled (killed intentionally)
15/04/18 02:59:20 WARN TaskSetManager: Lost task 15.0 in stage 33.0 (TID 1292, localhost): TaskKilled (killed intentionally)
15/04/18 02:59:20 WARN TaskSetManager: Lost task 23.0 in stage 33.0 (TID 1300, localhost): TaskKilled (killed intentionally)
15/04/18 02:59:20 WARN TaskSetManager: Lost task 21.0 in stage 33.0 (TID 1298, localhost): TaskKilled (killed intentionally)
15/04/18 02:59:20 WARN TaskSetManager: Lost task 17.0 in stage 33.0 (TID 1294, localhost): TaskKilled (killed intentionally)
15/04/18 02:59:20 WARN TaskSetManager: Lost task 2.0 in stage 33.0 (TID 1279, localhost): TaskKilled (killed intentionally)
15/04/18 02:59:20 WARN TaskSetManager: Lost task 0.0 in stage 33.0 (TID 1277, localhost): TaskKilled (killed intentionally)
15/04/18 02:59:20 WARN TaskSetManager: Lost task 16.0 in stage 33.0 (TID 1293, localhost): TaskKilled (killed intentionally)
15/04/18 02:59:20 WARN TaskSetManager: Lost task 25.0 in stage 33.0 (TID 1302, localhost): TaskKilled (killed intentionally)
15/04/18 02:59:20 WARN TaskSetManager: Lost task 9.0 in stage 33.0 (TID 1286, localhost): TaskKilled (killed intentionally)
15/04/18 02:59:20 WARN TaskSetManager: Lost task 19.0 in stage 33.0 (TID 1296, localhost): TaskKilled (killed intentionally)
15/04/18 02:59:20 ERROR Executor: Exception in task 7.0 in stage 33.0 (TID 1284)
java.io.FileNotFoundException: /tmp/spark-b105bf22-9cb9-4278-9e36-ff0b5f51c3dd/blockmgr-8a816f41-302e-4ae9-8078-f1f53af01915/06/shuffle_14_7_0.data (No such file or directory)
        at java.io.FileOutputStream.open(Native Method)
        at java.io.FileOutputStream.<init>(FileOutputStream.java:221)
        at org.apache.spark.storage.DiskBlockObjectWriter.open(BlockObjectWriter.scala:130)
        at org.apache.spark.storage.DiskBlockObjectWriter.write(BlockObjectWriter.scala:201)
        at org.apache.spark.util.collection.ExternalSorter$$anonfun$writePartitionedFile$5$$anonfun$apply$2.apply(ExternalSorter.scala:754)
        at org.apache.spark.util.collection.ExternalSorter$$anonfun$writePartitionedFile$5$$anonfun$apply$2.apply(ExternalSorter.scala:753)
        at scala.collection.Iterator$class.foreach(Iterator.scala:727)
        at org.apache.spark.util.collection.ExternalSorter$IteratorForPartition.foreach(ExternalSorter.scala:818)
        at org.apache.spark.util.collection.ExternalSorter$$anonfun$writePartitionedFile$5.apply(ExternalSorter.scala:753)
        at org.apache.spark.util.collection.ExternalSorter$$anonfun$writePartitionedFile$5.apply(ExternalSorter.scala:749)
        at scala.collection.Iterator$class.foreach(Iterator.scala:727)
        at scala.collection.AbstractIterator.foreach(Iterator.scala:1157)
        at org.apache.spark.util.collection.ExternalSorter.writePartitionedFile(ExternalSorter.scala:749)
        at org.apache.spark.shuffle.sort.SortShuffleWriter.write(SortShuffleWriter.scala:68)
        at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:68)
        at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
        at org.apache.spark.scheduler.Task.run(Task.scala:64)
        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:203)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:745)
15/04/18 02:59:20 WARN TaskSetManager: Lost task 7.0 in stage 33.0 (TID 1284, localhost): java.io.FileNotFoundException: /tmp/spark-b105bf22-9cb9-4278-9e36-ff0b5f51c3dd/blockmgr-8a816f41-302e-4ae9-8078-f1f53af01915/06/shuffle_14_7_0.data (No such file or directory)
        at java.io.FileOutputStream.open(Native Method)
        at java.io.FileOutputStream.<init>(FileOutputStream.java:221)
        at org.apache.spark.storage.DiskBlockObjectWriter.open(BlockObjectWriter.scala:130)
        at org.apache.spark.storage.DiskBlockObjectWriter.write(BlockObjectWriter.scala:201)
        at org.apache.spark.util.collection.ExternalSorter$$anonfun$writePartitionedFile$5$$anonfun$apply$2.apply(ExternalSorter.scala:754)
        at org.apache.spark.util.collection.ExternalSorter$$anonfun$writePartitionedFile$5$$anonfun$apply$2.apply(ExternalSorter.scala:753)
        at scala.collection.Iterator$class.foreach(Iterator.scala:727)
        at org.apache.spark.util.collection.ExternalSorter$IteratorForPartition.foreach(ExternalSorter.scala:818)
        at org.apache.spark.util.collection.ExternalSorter$$anonfun$writePartitionedFile$5.apply(ExternalSorter.scala:753)
        at org.apache.spark.util.collection.ExternalSorter$$anonfun$writePartitionedFile$5.apply(ExternalSorter.scala:749)
        at scala.collection.Iterator$class.foreach(Iterator.scala:727)
        at scala.collection.AbstractIterator.foreach(Iterator.scala:1157)
        at org.apache.spark.util.collection.ExternalSorter.writePartitionedFile(ExternalSorter.scala:749)
        at org.apache.spark.shuffle.sort.SortShuffleWriter.write(SortShuffleWriter.scala:68)
        at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:68)
        at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
        at org.apache.spark.scheduler.Task.run(Task.scala:64)
        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:203)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:745)
